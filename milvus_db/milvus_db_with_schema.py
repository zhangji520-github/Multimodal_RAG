"""Milvuså‘é‡æ•°æ®åº“æ“ä½œæ¨¡å—.
æä¾›Milvuså‘é‡æ•°æ®åº“çš„é›†åˆåˆ›å»ºã€è¿æ¥å’Œæ–‡æ¡£æ·»åŠ åŠŸèƒ½ã€‚
æ”¯æŒç¨ å¯†å‘é‡å’Œç¨€ç–å‘é‡çš„æ··åˆç´¢å¼•ã€‚
"""
import os
import sys
from typing import List, Optional, Dict

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from splitters.splitter_md import MarkdownDirSplitter
from langchain_core.documents import Document
from utils.embeddings_utils import build_work_items, process_item_with_guard, limiter, RETRY_ON_429, MAX_429_RETRIES, BASE_BACKOFF
from langchain_milvus import Milvus
from pymilvus import DataType, Function, FunctionType, MilvusClient
from utils.embeddings_utils import WorkItem
from env_utils import COLLECTION_NAME, MILVUS_URI
import logging
import time
import random



# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class MilvusVectorSave:
    """
    å»ºç«‹ä¸€ä¸ª Milvus é›†åˆ åœ¨schemaä¸­ä¿ç•™äº†æœªæ¥éœ€è¦çš„å­—æ®µ ä½¿ç”¨æ··åˆç´¢å¼•ï¼ŒåŒ…å«ç¨€ç–å‘é‡å’Œç¨ å¯†å‘é‡å­—æ®µ 
    """
    def __init__(self):
        # ç±»å‹æ³¨è§£ï¼šæ˜ç¡®å£°æ˜å±æ€§ç±»å‹ï¼Œæä¾›IDEæ™ºèƒ½æç¤ºå’Œç±»å‹æ£€æŸ¥
        self.vector_stored_saved: Optional[Milvus] = None
        self.client = MilvusClient(uri=MILVUS_URI, user='root', password='Milvus')

    def create_collection(self,collection_name: str = COLLECTION_NAME, uri: str = MILVUS_URI, is_first: bool = False):
        """åˆ›å»ºä¸€ä¸ªcollection milvus + langchain"""

        # 2. å®šä¹‰schema
        schema = self.client.create_schema()

        schema.add_field("id", DataType.INT64, is_primary=True, auto_id=True, description="ä¸»é”®")
        schema.add_field("category", DataType.VARCHAR, max_length=1000, description="å¯¹åº”å…ƒæ•°æ®çš„'embedding_type'")     
        schema.add_field("filename", DataType.VARCHAR, max_length=1000, description="å¯¹åº”å…ƒæ•°æ®çš„'source',æ–‡ä»¶å,å¸¦åç¼€")     
        schema.add_field("filetype", DataType.VARCHAR, max_length=1000, description="å¯¹åº”å…ƒæ•°æ®çš„'filetype',pdfæˆ–è€…md")     

        schema.add_field("title", DataType.VARCHAR, max_length=1000, enable_analyzer=True, 
                        analyzer_params={'tokenizer': 'jieba', 'filter': ['cnalphanumonly']}, description="å¯¹åº”å…ƒæ•°æ®çš„Header")
        schema.add_field("text", DataType.VARCHAR, max_length=6000, enable_analyzer=True,
                        analyzer_params={'tokenizer': 'jieba', 'filter': ['cnalphanumonly']}, description="å¯¹åº”æ¯ä¸ªæ–‡æœ¬å—çš„å†…å®¹") 
        schema.add_field("image_path", DataType.VARCHAR, max_length=2000, description="å›¾ç‰‡æ–‡ä»¶çš„æœ¬åœ°è·¯å¾„ï¼Œä»…å›¾ç‰‡ç±»å‹æ•°æ®ä½¿ç”¨")

        schema.add_field("title_sparse", DataType.SPARSE_FLOAT_VECTOR, description="æ ‡é¢˜çš„ç¨€ç–å‘é‡åµŒå…¥")
        schema.add_field("text_content_sparse", DataType.SPARSE_FLOAT_VECTOR, description="æ–‡æ¡£å—çš„ç¨€ç–å‘é‡åµŒå…¥")
        schema.add_field("text_content_dense", DataType.FLOAT_VECTOR, dim=1024, description="æ–‡æ¡£å—çš„ç¨ å¯†å‘é‡åµŒå…¥")

        logger.info(f'ğŸ¶æ·»åŠ schemaå®Œæˆ,å…±æ·»åŠ {len(schema.fields)}ä¸ªå­—æ®µ')

        # 3 ç¨€ç–å‘é‡éœ€è¦çš„bm25å‡½æ•°

        title_bm25_function = Function(
            name = "title_bm25_emb",
            input_field_names=["title"], # éœ€è¦è¿›è¡Œæ–‡æœ¬åˆ°ç¨€ç–å‘é‡è½¬æ¢çš„ VARCHAR å­—æ®µåç§°ã€‚
            output_field_names=["title_sparse"], # å­˜å‚¨å†…éƒ¨ç”Ÿæˆç¨€ç–å‘é‡çš„å­—æ®µåç§°ã€‚
            function_type=FunctionType.BM25 # è¦ä½¿ç”¨çš„å‡½æ•°ç±»å‹ã€‚
        )
        schema.add_function(title_bm25_function)          # bm25 æ­¤åŠŸèƒ½ä¼šæ ¹æ®æ–‡æœ¬çš„è¯­è¨€æ ‡è¯†è‡ªåŠ¨åº”ç”¨ç›¸åº”çš„åˆ†æå™¨

        content_bm25_function = Function(
            name = "text_content_bm25_emb",
            input_field_names=["text"], # éœ€è¦è¿›è¡Œæ–‡æœ¬åˆ°ç¨€ç–å‘é‡è½¬æ¢çš„ VARCHAR å­—æ®µåç§°ã€‚
            output_field_names=["text_content_sparse"], # å­˜å‚¨å†…éƒ¨ç”Ÿæˆç¨€ç–å‘é‡çš„å­—æ®µåç§°ã€‚
            function_type=FunctionType.BM25 # è¦ä½¿ç”¨çš„å‡½æ•°ç±»å‹ã€‚
        )
        schema.add_function(content_bm25_function)          # bm25 æ­¤åŠŸèƒ½ä¼šæ ¹æ®æ–‡æœ¬çš„è¯­è¨€æ ‡è¯†è‡ªåŠ¨åº”ç”¨ç›¸åº”çš„åˆ†æå™¨

        # 4 åˆ›å»ºç´¢å¼•å‚æ•°å¯¹è±¡
        try:
            logger.info("å¼€å§‹åˆ›å»ºç´¢å¼•å‚æ•°...")
            index_params = self.client.prepare_index_params()

            # ä¸»é”®ç´¢å¼•
            index_params.add_index(
                field_name="id",
                index_type="AUTOINDEX",
            )

            # ç¨€ç–å‘é‡ç´¢å¼• - æ ‡é¢˜
            index_params.add_index(
                field_name="title_sparse",
                index_type="SPARSE_INVERTED_INDEX",
                metric_type="BM25",
                params={
                    "inverted_index_algo": "DAAT_MAXSCORE",  # ç®—æ³•é€‰æ‹©
                    "bm25_k1": 1.2,  # è¯é¢‘é¥±å’Œåº¦æ§åˆ¶å‚æ•°
                    "bm25_b": 0.75  # æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–å‚æ•°
                }
            )

            # ç¨€ç–å‘é‡ç´¢å¼• -æ–‡æœ¬å—
            index_params.add_index(
                field_name="text_content_sparse",
                index_type="SPARSE_INVERTED_INDEX",
                metric_type="BM25",
                params={
                    "inverted_index_algo": "DAAT_MAXSCORE",  # ç®—æ³•é€‰æ‹©
                    "bm25_k1": 1.2,  # è¯é¢‘é¥±å’Œåº¦æ§åˆ¶å‚æ•°
                    "bm25_b": 0.75  # æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–å‚æ•°
                }
            )

            # ç¨ å¯†å‘é‡ç´¢å¼• - æ–‡æœ¬å—
            index_params.add_index(
                field_name="text_content_dense",
                index_type="HNSW",  # é€‚åˆç¨ å¯†å‘é‡çš„ç´¢å¼•ç±»å‹
                metric_type="COSINE",  # ä½™å¼¦ç›¸ä¼¼åº¦
                params={
                    "M": 16,  # HNSWå›¾ä¸­æ¯ä¸ªèŠ‚ç‚¹çš„æœ€å¤§è¿æ¥æ•°
                    "efConstruction": 200  # æ„å»ºç´¢å¼•æ—¶çš„æœç´¢å€™é€‰æ•°
                }
            )

            logger.info("ğŸ¶æˆåŠŸæ·»åŠ ç¨€ç–å‘é‡ç´¢å¼•å’Œç¨ å¯†å‘é‡ç´¢å¼•")

        except Exception as e:
            logger.error(f"ğŸ¶åˆ›å»ºç´¢å¼•å‚æ•°å¤±è´¥: {e}")

        #  5. åˆ›å»ºé›†åˆ
        # æ£€æŸ¥é›†åˆæ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨å…ˆé‡Šæ”¾collectionï¼Œç„¶åå†åˆ é™¤ç´¢å¼•å’Œé›†åˆ  
        if is_first:  
            if COLLECTION_NAME in self.client.list_collections():
                self.client.release_collection(collection_name=COLLECTION_NAME)
                self.client.drop_collection(collection_name=COLLECTION_NAME)

        self.client.create_collection(
            collection_name=COLLECTION_NAME,
            schema=schema,
            index_params=index_params,
        )
        logger.info(f"ğŸ¶æˆåŠŸåˆ›å»ºé›†åˆ: {COLLECTION_NAME}")
    
    @staticmethod
    def doc_to_dict(docs: List[Document]) -> List[Dict]:
        """
        å°†Documentåˆ—è¡¨è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼çš„å­—å…¸
        Args:
            docs: åŒ…å« Document å¯¹è±¡çš„åˆ—è¡¨  Document(page_content='', metadata={'source': 'pdf', 'embedding_type': 'text/image/video'}, 'image_path': 'path/to/image.jpg', 'Header1':'...', 'Header2':'...', 'Header3':'...')
        Returns:
            List[Dict]: æŒ‡å®šæ ¼å¼çš„å­—å…¸åˆ—è¡¨
        """
        result_dict = []

        for doc in docs:
            # åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—å…¸å­˜å‚¨å½“å‰æ–‡æ¡£ä¿¡æ¯
            doc_dict = {}
            metadata = doc.metadata
            
            # 1. æå–text (ä»…å½“embedding_typeä¸ºtext)
            if metadata.get('embedding_type') == 'text':
                doc_dict['text'] = doc.page_content
            else:
                doc_dict['text'] = ''       # # å›¾ç‰‡ç±»å‹åˆå§‹è®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²
            
            # 2. æå– category (embedding_type)
            doc_dict['category'] = metadata.get('embedding_type', '')
            
            # 3. æå– filename å’Œ filetype (pdf/md ä¹Ÿå°±æ˜¯ sourceçš„æ–‡ä»¶ååç¼€)
            source = metadata.get('source', '')
            doc_dict['filename'] = source
            _, ext = os.path.splitext(source)
            doc_dict['filetype'] = ext[1:].lower() if ext.startswith('.') else ext.lower()
            
            # 4.  å›¾ç‰‡ä¸“ç”¨å¤„ç† æå– image_path (ä»…å½“ embedding_type ä¸º 'image' æ—¶)
            if metadata.get('embedding_type') == 'image':
                doc_dict['image_path'] = doc.page_content        # å›¾ç‰‡è·¯å¾„å­˜å‚¨åœ¨image_path
                doc_dict['text'] = 'å›¾ç‰‡'  # è¦†ç›–ä¹‹å‰çš„ç©ºå­—ç¬¦ä¸²ï¼Œè®¾ç½®ä¸º"å›¾ç‰‡"
            else:
                doc_dict['image_path'] = ''
            
            # 5. å¯¹äºæ–‡æœ¬å—  æå– title(æ‹¼æ¥æ‰€æœ‰çš„ Header) ä¸å†…å®¹(doc.page_content) å­˜å‚¨åˆ° text å­—æ®µ
            headers = []
            # å‡è®¾ Header çš„é”®å¯èƒ½ä¸º 'Header 1', 'Header 2', 'Header 3' ç­‰ï¼Œæˆ‘ä»¬æŒ‰å±‚çº§é¡ºåºæ‹¼æ¥
            # æˆ‘ä»¬éœ€è¦å…ˆæ”¶é›†æ‰€æœ‰å­˜åœ¨çš„ Header é”®ï¼Œå¹¶æŒ‰å±‚çº§æ’åº
            header_keys = [key for key in metadata.keys() if key.startswith('Header')]              # ['Header 1', 'Header 3']
            # æŒ‰ Header åçš„æ•°å­—æ’åºï¼Œç¡®ä¿å±‚çº§é¡ºåº
            header_keys_sorted = sorted(header_keys, key=lambda x: int(x.split()[1]) if x.split()[1].isdigit() else x)

            for key in header_keys_sorted:
                value = metadata.get(key, '').strip()
                if value:  # åªæ·»åŠ éç©ºçš„ Header å€¼
                    headers.append(value)
            
            # å°†æ‰€æœ‰éç©ºçš„ Header å€¼ç”¨è¿å­—ç¬¦æˆ–ç©ºæ ¼è¿æ¥èµ·æ¥
            doc_dict['title'] = ' --> '.join(headers) if headers else ''  # ä½ ä¹Ÿå¯ä»¥ç”¨å…¶ä»–è¿æ¥ç¬¦ï¼Œå¦‚ç©ºæ ¼
            # å¯¹æ–‡æœ¬å—å¤„ç†ï¼šæ‹¼æ¥æ ‡é¢˜å’Œå†…å®¹
            if metadata.get('embedding_type') == 'text':
                if doc_dict['title']:
                    doc_dict['text'] = doc_dict['title'] + 'ï¼š' + doc.page_content
                else:
                    doc_dict['text'] = doc.page_content
            
            # 6. å°†doc_dictæ·»åŠ åˆ°result_dictä¸­
            result_dict.append(doc_dict)
            
        return result_dict
    
    def write_to_milvus(self, processed_data: List[Dict]):
        """
        æŠŠæ•°æ®å†™å…¥åˆ°Milvusä¸­
        :param processed_data:
        :return:
        """
        if not processed_data:
            logger.warning("ğŸ¶æ²¡æœ‰éœ€è¦å†™å…¥çš„æ•°æ®")
            return
        
        try:
            insert_res = self.client.insert(collection_name=COLLECTION_NAME, data=processed_data)
            print(f"[Milvus] æˆåŠŸå†™å…¥ {len(processed_data)} æ¡æ•°æ®.IDs ç¤ºä¾‹: {insert_res['ids'][:5]}")
        except Exception as e:
            logger.error(f"ğŸ¶å†™å…¥Milvuså¤±è´¥: {e}")
            raise e
        
    def do_save_to_milvus(self, docs: List[Document]):
        """
        å°† LangChain çš„ Document å¯¹è±¡åˆ—è¡¨ï¼ˆæ¥è‡ªæ–‡æœ¬/å›¾åƒåˆ†å‰²å™¨ï¼‰
        â†’ è½¬æ¢ä¸ºç»“æ„åŒ–å­—å…¸
        â†’ è°ƒç”¨ DashScope å¤šæ¨¡æ€ API ç”Ÿæˆç»Ÿä¸€å‘é‡ï¼ˆdenseï¼‰
        â†’ å†™å…¥ Milvus å‘é‡æ•°æ®åº“ 
        è¿”å›çš„ result æ˜¯å¢å¼ºåçš„å­—å…¸ï¼Œå¿…å®šåŒ…å« dense å­—æ®µï¼š
            æˆåŠŸ dense = [0.1, -0.2, ...]
            å¤±è´¥ dense = []
            å›¾åƒä»»åŠ¡è¿˜ä¼šè®¾ç½® text = "å›¾ç‰‡"ï¼ˆä¾¿äºå‰ç«¯å±•ç¤ºï¼‰
        """
        # ç¬¬ä¸€æ­¥ï¼šæŠŠ Document è½¬æ¢ä¸ºç»“æ„åŒ–å­—å…¸
        expanded_data = self.doc_to_dict(docs)
        # ç¬¬äºŒæ­¥ï¼šè°ƒç”¨ DashScope å¤šæ¨¡æ€ API ç”Ÿæˆç»Ÿä¸€å‘é‡ï¼ˆdenseï¼‰
        work_items: List[WorkItem] = build_work_items(expanded_data)
        
        processed_data: List[Dict] = []
        for idx,wi in enumerate(work_items, start=1):
            limiter.acquire()

            # æƒ…å†µä¸€ï¼šå¯ç”¨ 429 é™æµé‡è¯•ï¼ˆRETRY_ON_429 = Trueï¼‰
            if RETRY_ON_429:
                attempts = 0
                while True:
                    res = process_item_with_guard(wi.item.copy(), wi.mode, wi.api_image)
                    if res.get('text_content_dense'):
                        processed_data.append(res)
                        break
                    attempts += 1
                    if attempts >= MAX_429_RETRIES:
                        print(f"ğŸ¶429é‡è¯•æ¬¡æ•°è¶…è¿‡æœ€å¤§å€¼: {MAX_429_RETRIES}")
                        processed_data.append(res)
                        break
                    backoff = BASE_BACKOFF * (2 ** (attempts - 1)) * (0.8 + random.random() * 0.4)
                    print(f"[429é‡è¯•] ç¬¬{attempts}æ¬¡ï¼Œsleep {backoff:.2f}s â€¦")
                    time.sleep(backoff)
            # æƒ…å†µäºŒï¼šä¸å¯ç”¨ 429 é™æµé‡è¯•ï¼ˆRETRY_ON_429 = Falseï¼‰ é€‚ç”¨äºè°ƒè¯•æˆ–ä½é¢‘åœºæ™¯
            else:
                result = process_item_with_guard(wi.item.copy(), mode=wi.mode, api_image=wi.api_image)
                processed_data.append(result)
            # è¿›åº¦æç¤º æ¯ 20 ä¸ª item æ‰“å°ä¸€æ¬¡è¿›åº¦ï¼Œé¿å…æ—¥å¿—åˆ·å±
            if idx % 20 == 0:
                print(f"[è¿›åº¦] å·²å¤„ç† {idx}/{len(work_items)}")

        # ç¬¬ä¸‰æ­¥
        self.write_to_milvus(processed_data)
        return processed_data

if __name__ == "__main__":

    # åˆ›å»ºè¡¨ç»“æ„
    milvus_vector_save = MilvusVectorSave()
    milvus_vector_save.create_collection(is_first=True)
    
    # æŸ¥çœ‹é›†åˆä¿¡æ¯
    # client = MilvusClient(uri=MILVUS_URI, user='root', password='Milvus')
    # res = client.describe_collection(collection_name=COLLECTION_NAME)
    # print("é›†åˆä¿¡æ¯:")
    # print(res)
    md_dir = r"F:\workspace\langgraph_project\Multimodal_RAG\output\RBFç¥ç»ç½‘ç»œæ— äººè‰‡åŒ…å«æ§åˆ¶æ¨å¯¼"
    splitter = MarkdownDirSplitter(images_output_dir=r"F:\workspace\langgraph_project\Multimodal_RAG\output\images")
    docs = splitter.process_md_dir(md_dir, source_filename="RBFç¥ç»ç½‘ç»œæ— äººè‰‡åŒ…å«æ§åˆ¶æ¨å¯¼.pdf")

    res: List[Dict] = milvus_vector_save.do_save_to_milvus(docs)

    # æ‰“å°
    # æ‰“å°å…³é”®æ•°æ®
    for i, item in enumerate(res):
        print(f"\n==== ç¬¬{i+1}æ¡æ•°æ® ====")
        # æ‰“å°æ–‡æœ¬å†…å®¹å‰30å­—
        text = item.get('text', '')
        print(f"å†…å®¹: {text[:30]}{'...' if len(text) > 30 else ''}")
        # æ‰“å°æ ‡é¢˜
        print(f"æ ‡é¢˜: {item.get('title', '')}")
        # æ‰“å°æ–‡ä»¶åã€æ–‡ä»¶ç±»å‹
        print(f"æ–‡ä»¶å: {item.get('filename', '')}")
        print(f"æ–‡ä»¶ç±»å‹: {item.get('filetype', '')}")

